# ----

# ì‘ì„±ëª©ì  : í†µí•© ì˜ìƒ ë¶„ì„ API ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
# ì‘ì„±ì¼ : 2025-06-14

# ë³€ê²½ì‚¬í•­ ë‚´ì—­ (ë‚ ì§œ | ë³€ê²½ëª©ì  | ë³€ê²½ë‚´ìš© | ì‘ì„±ì ìˆœìœ¼ë¡œ ê¸°ì…)
# 2025-06-14 | ìµœì´ˆ êµ¬í˜„ | FastAPI ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ì— ë”°ë¥¸ êµ¬ì¡°ë¡œ ì¬êµ¬ì„± | ì´ì¬ì¸
# 2025-06-16 | êµ¬ì¡° ê°œì„  | DB ì €ì¥, S3 ì—°ë™, LLM ì—°ë™ êµ¬ì¡° ìµœì í™” | ì´ì¬ì¸
# 2025-06-16 | ìë™ ë¶„ì„ | ì„œë²„ ì‹œì‘ ì‹œ S3 ëª¨ë“  ì˜ìƒ ìë™ ë¶„ì„ ê¸°ëŠ¥ ì¶”ê°€ | ì´ì¬ì¸
# 2025-06-17 | ê¸°ëŠ¥ ìµœì í™” | ìë™ ë¶„ì„ ê¸°ëŠ¥ë§Œ ë‚¨ê¸°ê³  ìˆ˜ë™ ì—…ë¡œë“œ ê´€ë ¨ ê¸°ëŠ¥ ì‚­ì œ | ì´ì¬ì¸
# ----

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import Optional, Dict, Any, List
import asyncio
import os
import tempfile
import json
import shutil
from datetime import datetime
from dotenv import load_dotenv
import sys

# í˜„ì¬ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

from src.utils.s3_handler import S3Handler
from src.utils.file_utils import FileProcessor
from src.emotion.analyzer import EmotionAnalyzer
from src.eye_tracking.analyzer import EyeTrackingAnalyzer
from src.db.database import get_db_session
from src.db.crud import save_analysis_result, get_analysis_results
from src.llm.gpt_analyzer import GPTAnalyzer
from src.db.mariadb_handler import mariadb_handler

# --- Pydantic ëª¨ë¸ ì •ì˜ ---
class AnalysisPayload(BaseModel):
    """ë©”ì¸ ì„œë²„ë¡œë¶€í„° ë¶„ì„ ìš”ì²­ì„ ìˆ˜ì‹ í•  ëª¨ë¸"""
    s3ObjectKey: str

app = FastAPI(
    title="í†µí•© ì˜ìƒ ë¶„ì„ API",
    description="API ìš”ì²­ì„ í†µí•´ S3 ì˜ìƒì„ ë¶„ì„í•˜ì—¬ ê°ì • ë° ì‹œì„  ì¶”ì  ê²°ê³¼ë¥¼ ì œê³µí•˜ëŠ” API",
    version="2.1.0"
)

# ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì „ì—­ ë³€ìˆ˜
_pending_gpt_analyses = []  # GPT ë¶„ì„ ëŒ€ê¸° í
_batch_processing_active = False  # ë°°ì¹˜ ì²˜ë¦¬ í™œì„±í™” ìƒíƒœ

@app.on_event("startup")
async def startup_event():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ì‹¤í–‰ë˜ëŠ” ì´ë²¤íŠ¸"""
    try:
        # MariaDB ì—°ê²° í’€ ìƒì„±
        await mariadb_handler.create_pool()
        print("âœ… MariaDB ì—°ê²°ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("ğŸš€ ë¶„ì„ ì„œë²„ê°€ ì„±ê³µì ìœ¼ë¡œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. API ìš”ì²­ì„ ëŒ€ê¸°í•©ë‹ˆë‹¤.")
        
        # âš ï¸ S3 ìë™ ë¶„ì„ ë¡œì§ ì œê±°
        # print("ğŸ“¡ S3 ìë™ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        # asyncio.create_task(auto_analyze_all_s3_videos())
        
    except Exception as e:
        print(f"âš ï¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ğŸ“ MariaDB ì—°ê²° ì‹¤íŒ¨ - MongoDBë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")

@app.on_event("shutdown")
async def shutdown_event():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ ì‹œ ì‹¤í–‰ë˜ëŠ” ì´ë²¤íŠ¸"""
    try:
        await mariadb_handler.close_pool()
        print("âœ… ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ì •ìƒì ìœ¼ë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âš ï¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ìš”ì²­ ëª¨ë¸ (ìë™ ë¶„ì„ ê´€ë ¨ë§Œ)
class S3UserVideoRequest(BaseModel):
    """S3 ì‚¬ìš©ìë³„ ì˜ìƒ ë¶„ì„ ìš”ì²­"""
    user_id: str  # ì‚¬ìš©ì ID (ì˜ˆ: "iv001", "user123")
    question_num: str  # ì§ˆë¬¸ ë²ˆí˜¸ (ì˜ˆ: "Q001", "1", "question_1")
    session_id: Optional[str] = None

class AnalysisResponse(BaseModel):
    analysis_id: str
    status: str
    message: str
    result: Optional[Dict[str, Any]] = None

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
s3_handler = S3Handler()
file_processor = FileProcessor()
emotion_analyzer = EmotionAnalyzer()
eye_tracking_analyzer = EyeTrackingAnalyzer()
gpt_analyzer = GPTAnalyzer()

# --- ì˜ìƒ ìˆ˜ì‹  API ì—”ë“œí¬ì¸íŠ¸ ---
@app.post("/analysis/attitude", response_model=AnalysisResponse)
async def analyze_video_from_s3_key(payload: AnalysisPayload):
    """
    ë©”ì¸ ì„œë²„ë¡œë¶€í„° S3 Object Keyë¥¼ ë°›ì•„ ì˜ìƒ ë¶„ì„ì„ ì™„ë£Œí•œ í›„ ê²°ê³¼ë¥¼ ì‘ë‹µí•©ë‹ˆë‹¤.
    """
    try:
        s3_key = payload.s3ObjectKey
        print(f"Received analysis request for s3ObjectKey: {s3_key}")
        
        # S3 í‚¤ë¡œë¶€í„° ì‚¬ìš©ì IDì™€ ì§ˆë¬¸ ë²ˆí˜¸ ì¶”ì¶œ
        try:
            # ì‹¤ì œ S3 í‚¤ í˜•ì‹: skala25a/team12/interview_video/{userId}/{question_num}/*.webm
            parts = s3_key.split('/')
            print(f"ğŸ” S3 í‚¤ ë¶„í• : {parts}")
            
            # interview_video ë‹¤ìŒì— ì˜¤ëŠ” ê²½ë¡œì—ì„œ user_idì™€ question_num ì¶”ì¶œ
            if 'interview_video' in parts:
                video_index = parts.index('interview_video')
                if video_index + 2 < len(parts):
                    user_id = parts[video_index + 1]
                    question_num = parts[video_index + 2]
                    print(f"ğŸ” íŒŒì‹± ì„±ê³µ: user_id={user_id}, question_num={question_num}")
                else:
                    raise IndexError("interview_video ë‹¤ìŒì— user_idì™€ question_numì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                raise IndexError("S3 í‚¤ì— 'interview_video' ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤.")
                
        except (IndexError, ValueError) as e:
            raise HTTPException(
                status_code=400,
                detail=f"ì˜ëª»ëœ S3 í‚¤ í˜•ì‹ì…ë‹ˆë‹¤. 'skala25a/team12/interview_video/{{user_id}}/{{question_num}}/...' í˜•ì‹ì„ ê¸°ëŒ€í•©ë‹ˆë‹¤: {s3_key}"
            )
            
        analysis_id = f"api_s3_analysis_{user_id}_{question_num}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        bucket_name = os.getenv('S3_BUCKET_NAME', 'skala25a')
        session_id = f"api_triggered_{user_id}"

        print(f"ğŸ¬ API ìš”ì²­ ê¸°ë°˜ ë¶„ì„ ì‹œì‘: {user_id}/{question_num} -> {s3_key}")
        
        # ëª¨ë“  ë¶„ì„ ì‘ì—…ì„ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰í•˜ê³  ì™„ë£Œê¹Œì§€ ëŒ€ê¸°
        analysis_result = await process_s3_user_video_analysis(
            analysis_id=analysis_id,
            s3_bucket=bucket_name,
            s3_key=s3_key,
            user_id=user_id,
            question_num=question_num,
            session_id=None
        )
        
        # ë¶„ì„ ê²°ê³¼ í™•ì¸
        if analysis_result and "error" not in analysis_result:
            return AnalysisResponse(
                analysis_id=analysis_id,
                status="completed",
                message=f"ì‚¬ìš©ì {user_id}, ì§ˆë¬¸ {question_num} ì˜ìƒ ë¶„ì„ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
                result=analysis_result
            )
        else:
            error_msg = analysis_result.get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜") if analysis_result else "ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"
            return AnalysisResponse(
                analysis_id=analysis_id,
                status="failed",
                message=f"ì˜ìƒ ë¶„ì„ ì‹¤íŒ¨: {error_msg}",
                result=analysis_result
            )

    except HTTPException as http_exc:
        raise http_exc
    except Exception as e:
        # ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬ì— ëŒ€í•œ ë¡œê¹… ê°•í™”
        print(f"ğŸ”¥ /analyze ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ë¶„ì„ ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì„œë²„ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.get("/")
async def root():
    """API ìƒíƒœ í™•ì¸"""
    return {"message": "í†µí•© ì˜ìƒ ë¶„ì„ API (ìë™ ë¶„ì„ ì „ìš©)ê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤."}

@app.get("/s3/available-users-questions")
async def get_available_users_and_questions():
    """
    S3ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì‚¬ìš©ìì™€ ì§ˆë¬¸ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    try:
        bucket_name = os.getenv('S3_BUCKET_NAME', 'skala25a')
        available_videos = await s3_handler.list_available_users_and_questions(bucket_name)
        
        return {
            "bucket": bucket_name,
            "available_videos": available_videos,
            "total_users": len(available_videos),
            "total_videos": sum(len(questions) for questions in available_videos.values())
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"S3 ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.get("/s3/find-video/{user_id}/{question_num}")
async def find_specific_video(user_id: str, question_num: str):
    """
    íŠ¹ì • ì‚¬ìš©ì/ì§ˆë¬¸ì˜ ì˜ìƒ íŒŒì¼ì„ S3ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    """
    try:
        bucket_name = os.getenv('S3_BUCKET_NAME', 'skala25a')
        video_key = await s3_handler.find_video_file(bucket_name, user_id, question_num)
        
        if video_key:
            return {
                "found": True,
                "user_id": user_id,
                "question_num": question_num,
                "s3_key": video_key,
                "s3_url": f"s3://{bucket_name}/{video_key}"
            }
        else:
            return {
                "found": False,
                "user_id": user_id,
                "question_num": question_num,
                "message": "í•´ë‹¹ ì‚¬ìš©ì/ì§ˆë¬¸ì˜ ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì˜ìƒ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")

@app.post("/analyze-s3-user-video", response_model=AnalysisResponse)
async def analyze_s3_user_video(request: S3UserVideoRequest, background_tasks: BackgroundTasks):
    """
    S3ì—ì„œ íŠ¹ì • ì‚¬ìš©ì/ì§ˆë¬¸ì˜ ì˜ìƒì„ ë¶„ì„í•©ë‹ˆë‹¤. (ìˆ˜ë™ íŠ¸ë¦¬ê±°)
    """
    try:
        # ë¶„ì„ ID ìƒì„±
        analysis_id = f"manual_s3_analysis_{request.user_id}_{request.question_num}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # S3 ì„¤ì •
        bucket_name = os.getenv('S3_BUCKET_NAME', 'skala25a')
        
        # ì˜ìƒ íŒŒì¼ ê²€ìƒ‰
        video_key = await s3_handler.find_video_file(bucket_name, request.user_id, request.question_num)
        
        if not video_key:
            raise HTTPException(
                status_code=404, 
                detail=f"ì‚¬ìš©ì {request.user_id}, ì§ˆë¬¸ {request.question_num}ì˜ ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
        
        print(f"ğŸ¬ ìˆ˜ë™ ë¶„ì„ ì‹œì‘: {request.user_id}/{request.question_num} -> {video_key}")
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¶„ì„ ì‹¤í–‰
        background_tasks.add_task(
            process_s3_user_video_analysis,
            analysis_id,
            bucket_name,
            video_key,
            request.user_id,
            request.question_num,
            request.session_id or "manual"
        )
        
        return AnalysisResponse(
            analysis_id=analysis_id,
            status="processing",
            message=f"ì‚¬ìš©ì {request.user_id}, ì§ˆë¬¸ {request.question_num} ì˜ìƒ ë¶„ì„ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¶„ì„ ì‹œì‘ ì‹¤íŒ¨: {str(e)}")

@app.get("/analysis/{analysis_id}")
async def get_analysis_result(analysis_id: str):
    """
    ë¶„ì„ ê²°ê³¼ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    try:
        with get_db_session() as db:
            collection = db['analysis_results']
            result = collection.find_one({"analysis_id": analysis_id})
            
            if not result:
                raise HTTPException(status_code=404, detail="ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # ObjectIdë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            if '_id' in result:
                result['_id'] = str(result['_id'])
            
            return result
            
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¶„ì„ ê²°ê³¼ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.get("/analysis/{analysis_id}/llm-comment")
async def get_llm_comment(analysis_id: str):
    """
    íŠ¹ì • ë¶„ì„ì˜ LLM ì½”ë©˜íŠ¸ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    try:
        with get_db_session() as db:
            collection = db['llm_comments']
            comment = collection.find_one({"analysis_id": analysis_id})
            
            if not comment:
                return {"message": "LLM ì½”ë©˜íŠ¸ê°€ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
            
            # ObjectIdë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            if '_id' in comment:
                comment['_id'] = str(comment['_id'])
            
            return comment
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"LLM ì½”ë©˜íŠ¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.get("/analysis/recent")
async def get_recent_analyses(limit: int = 10):
    """
    ìµœê·¼ ë¶„ì„ ê²°ê³¼ë“¤ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    try:
        with get_db_session() as db:
            collection = db['analysis_results']
            
            results = []
            for doc in collection.find().sort("created_at", -1).limit(limit):
                # ObjectIdë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
                if '_id' in doc:
                    doc['_id'] = str(doc['_id'])
                results.append(doc)
            
            return {"recent_analyses": results, "count": len(results)}
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ìµœê·¼ ë¶„ì„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.get("/analysis/{analysis_id}/status")
async def get_analysis_status(analysis_id: str):
    """
    ë¶„ì„ ì§„í–‰ ìƒíƒœë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    try:
        with get_db_session() as db:
            collection = db['analysis_results']
            result = collection.find_one(
                {"analysis_id": analysis_id},
                {"analysis_id": 1, "status": 1, "progress": 1, "stage": 1, "created_at": 1, "completed_at": 1}
            )
            
            if not result:
                raise HTTPException(status_code=404, detail="ë¶„ì„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # ObjectIdë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            if '_id' in result:
                result['_id'] = str(result['_id'])
            
            return result
            
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¶„ì„ ìƒíƒœ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.post("/analysis/{analysis_id}/cancel")
async def cancel_analysis(analysis_id: str):
    """
    ì§„í–‰ ì¤‘ì¸ ë¶„ì„ì„ ì·¨ì†Œí•©ë‹ˆë‹¤. (ì‹¤ì œë¡œëŠ” ìƒíƒœë§Œ ë³€ê²½)
    """
    try:
        with get_db_session() as db:
            collection = db['analysis_results']
            result = collection.update_one(
                {"analysis_id": analysis_id, "status": "processing"},
                {"$set": {"status": "cancelled", "cancelled_at": datetime.now().isoformat()}}
            )
            
            if result.matched_count == 0:
                raise HTTPException(status_code=404, detail="ì·¨ì†Œí•  ìˆ˜ ìˆëŠ” ë¶„ì„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            return {"message": "ë¶„ì„ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.", "analysis_id": analysis_id}
            
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¶„ì„ ì·¨ì†Œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.get("/interview-attitude/{user_id}")
async def get_interview_attitude_by_user(user_id: str):
    """
    íŠ¹ì • ì‚¬ìš©ìì˜ ëª¨ë“  ë©´ì ‘íƒœë„ í‰ê°€ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    try:
        scores = await mariadb_handler.get_interview_attitude(user_id)
        
        if not scores:
            return {"message": f"ì‚¬ìš©ì {user_id}ì˜ ë©´ì ‘íƒœë„ í‰ê°€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "scores": []}
        
        return {
            "user_id": user_id,
            "scores": scores,
            "count": len(scores) if isinstance(scores, list) else 1
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë©´ì ‘íƒœë„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.get("/interview-attitude/{user_id}/{question_num}")
async def get_interview_attitude_by_user_question(user_id: str, question_num: str):
    """
    íŠ¹ì • ì‚¬ìš©ì/ì§ˆë¬¸ì˜ ë©´ì ‘íƒœë„ í‰ê°€ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    try:
        score = await mariadb_handler.get_interview_attitude(user_id, question_num)
        
        if not score:
            raise HTTPException(
                status_code=404, 
                detail=f"ì‚¬ìš©ì {user_id}, ì§ˆë¬¸ {question_num}ì˜ ë©´ì ‘íƒœë„ í‰ê°€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )
        
        return score
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë©´ì ‘íƒœë„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

@app.get("/health")
async def health_check():
    """
    ì‹œìŠ¤í…œ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    """
    try:
        # MongoDB ì—°ê²° í™•ì¸
        mongodb_status = "healthy"
        try:
            with get_db_session() as db:
                db.list_collection_names()
        except Exception as e:
            mongodb_status = f"error: {str(e)}"
        
        # MariaDB ì—°ê²° í™•ì¸
        mariadb_status = "healthy"
        try:
            await mariadb_handler.test_connection()
        except Exception as e:
            mariadb_status = f"error: {str(e)}"
        
        # S3 ì—°ê²° í™•ì¸
        s3_status = "healthy"
        try:
            bucket_name = os.getenv('S3_BUCKET_NAME', 'skala25a')
            await s3_handler.test_connection(bucket_name)
        except Exception as e:
            s3_status = f"error: {str(e)}"
        
        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "services": {
                "mongodb": mongodb_status,
                "mariadb": mariadb_status,
                "s3": s3_status
            }
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {str(e)}")

@app.post("/test/yaml-keywords")
async def test_yaml_keywords(analysis_data: Dict[str, Any] = None):
    """YAML ê¸°ë°˜ í‚¤ì›Œë“œ ë¶„ì„ í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        from src.llm.keyword_analyzer import keyword_analyzer
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (ìš”ì²­ì— ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        if not analysis_data:
            analysis_data = {
                'emotion_score': 45,
                'eye_score': 28,
                'concentration_score': 15,
                'stability_score': 8,
                'blink_score': 5,
                'total_violations': 3,
                'face_multiple_detected': False,
                'suspected_copying': False,
                'suspected_impersonation': False,
                'dominant_emotions': 'neutral',
                'emotion_stability': 'ë³´í†µ'
            }
        
        # í‚¤ì›Œë“œ ë¶„ì„ ì‹¤í–‰
        result = keyword_analyzer.analyze_keywords(analysis_data)
        
        # GPT í”„ë¡¬í”„íŠ¸ë„ ìƒì„±í•´ë³´ê¸°
        system_prompt, user_prompt = keyword_analyzer.get_gpt_prompt(analysis_data)
        
        return {
            "status": "success",
            "keyword_analysis": result,
            "gpt_prompts": {
                "system_prompt": system_prompt[:500] + "..." if len(system_prompt) > 500 else system_prompt,
                "user_prompt": user_prompt[:500] + "..." if len(user_prompt) > 500 else user_prompt
            },
            "input_data": analysis_data
        }
        
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
            "input_data": analysis_data or {}
        }

@app.post("/test/mariadb-save")
async def test_mariadb_save(request: dict):
    """MariaDB ì €ì¥ í…ŒìŠ¤íŠ¸ (ìƒˆë¡œìš´ ID í˜•ì‹)"""
    try:
        user_id = request.get("user_id", "123")
        question_num = request.get("question_num", "1")
        emotion_score = request.get("emotion_score", 45.0)
        eye_score = request.get("eye_score", 28.0)
        suspected_copying = request.get("suspected_copying", False)
        suspected_impersonation = request.get("suspected_impersonation", False)
        gpt_analysis = request.get("gpt_analysis", {})
        
        # MariaDBì— ì €ì¥
        success = await mariadb_handler.save_interview_attitude(
            user_id=user_id,
            question_num=question_num,
            emotion_score=emotion_score,
            eye_score=eye_score,
            suspected_copying=suspected_copying,
            suspected_impersonation=suspected_impersonation,
            gpt_analysis=gpt_analysis
        )
        
        if success:
            # ì €ì¥ëœ ë°ì´í„° ì¡°íšŒ
            saved_data = await mariadb_handler.get_interview_attitude(user_id, question_num)
            
            return {
                "status": "success",
                "message": f"âœ… ë°ì´í„° ì €ì¥ ì™„ë£Œ (ìƒˆë¡œìš´ ID í˜•ì‹)",
                "request_data": request,
                "id_format": {
                    "ANS_SCORE_ID": f"{user_id}0{question_num}",
                    "INTV_ANS_ID": f"{user_id}0{question_num}", 
                    "ANS_CAT_RESULT_ID": f"{user_id}0{question_num}0"
                },
                "saved_data": saved_data
            }
        else:
            return {"status": "error", "message": "ë°ì´í„° ì €ì¥ ì‹¤íŒ¨"}
            
    except Exception as e:
        logger.error(f"MariaDB í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        return {"status": "error", "message": str(e)}

@app.get("/test/yaml-all-features")
async def test_yaml_all_features():
    """ëª¨ë“  YAML ê¸°ë°˜ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    try:
        from src.llm.keyword_analyzer import keyword_analyzer
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        test_emotion_result = {
            'interview_score': 45,
            'dominant_emotion': 'neutral',
            'emotion_ratios': {'happy': 0.3, 'neutral': 0.6, 'sad': 0.1},
            'detailed_analysis': {
                'scores': {'expressiveness': 30, 'stability': 25},
                'improvement_suggestions': ['í‘œì • ë‹¤ì–‘ì„± ê°œì„ ']
            },
            'total_frames': 1500,
            'emotion_counts': {'happy': 450, 'neutral': 900, 'sad': 150},
            'confidence_scores': {'happy': 0.85, 'neutral': 0.75, 'sad': 0.6},
            'grade': 'B'
        }
        
        test_eye_result = {
            'basic_scores': {
                'total_eye_score': 28,
                'concentration_score': 15,
                'stability_score': 8,
                'blink_score': 5
            },
            'analysis_summary': {
                'total_violations': 3,
                'face_multiple_detected': False,
                'center_time_ratio': 0.7
            },
            'total_duration': 60.0,
            'blink_count': 45,
            'blink_rate': 0.75,
            'attention_score': 25,
            'gaze_stability': 20,
            'focus_score': 22
        }
        
        analysis_data = {
            'emotion_score': 45,
            'eye_score': 28,
            'concentration_score': 15,
            'stability_score': 8,
            'blink_score': 5,
            'total_violations': 3,
            'face_multiple_detected': False,
            'suspected_copying': False,
            'suspected_impersonation': False,
            'dominant_emotions': 'neutral',
            'emotion_stability': 'ë³´í†µ'
        }
        
        # 1. í‚¤ì›Œë“œ ë¶„ì„ í…ŒìŠ¤íŠ¸
        keywords = keyword_analyzer.analyze_keywords(analysis_data)
        
        # 2. GPT í”„ë¡¬í”„íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸
        gpt_prompt = keyword_analyzer.get_gpt_prompt(analysis_data)
        
        # 3. ìƒì„¸ GPT í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸
        detailed_prompt = keyword_analyzer.get_detailed_gpt_prompt(test_emotion_result, test_eye_result)
        
        # 4. ë™ì  í”¼ë“œë°± ìƒì„± í…ŒìŠ¤íŠ¸
        dynamic_feedback = keyword_analyzer.generate_dynamic_feedback(test_emotion_result, test_eye_result)
        
        return {
            "status": "success",
            "message": "âœ… ëª¨ë“  YAML ê¸°ë°˜ ê¸°ëŠ¥ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤",
            "test_results": {
                "1_keyword_analysis": keywords,
                "2_gpt_prompt": {
                    "system": gpt_prompt[0][:300] + "..." if len(gpt_prompt[0]) > 300 else gpt_prompt[0],
                    "user": gpt_prompt[1][:300] + "..." if len(gpt_prompt[1]) > 300 else gpt_prompt[1]
                },
                "3_detailed_prompt": {
                    "system": detailed_prompt[0][:300] + "..." if len(detailed_prompt[0]) > 300 else detailed_prompt[0],
                    "user": detailed_prompt[1][:300] + "..." if len(detailed_prompt[1]) > 300 else detailed_prompt[1]
                },
                "4_dynamic_feedback": dynamic_feedback
            },
            "test_data": {
                "emotion_result": test_emotion_result,
                "eye_result": test_eye_result,
                "analysis_data": analysis_data
            }
        }
        
    except Exception as e:
        logger.error(f"YAML ì „ì²´ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return {"status": "error", "message": str(e)}

@app.get("/auto-analysis/status")
async def get_auto_analysis_status():
    """
    ìë™ ë¶„ì„ ì§„í–‰ ìƒí™©ì„ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    try:
        with get_db_session() as db:
            collection = db['analysis_results']
            
            # ì „ì²´ ë¶„ì„ ê²°ê³¼ í†µê³„
            total_analyses = collection.count_documents({})
            completed_analyses = collection.count_documents({"status": "completed"})
            processing_analyses = collection.count_documents({"status": "processing"})
            failed_analyses = collection.count_documents({"status": "error"})
            
            # ìë™ ë¶„ì„ ê²°ê³¼ (session_idê°€ "auto_batch"ì¸ ê²ƒë“¤)
            auto_analyses = collection.count_documents({"session_id": "auto_batch"})
            auto_completed = collection.count_documents({
                "session_id": "auto_batch", 
                "status": "completed"
            })
            
            # ìµœê·¼ ë¶„ì„ ê²°ê³¼ (ìµœê·¼ 10ê°œ)
            recent_analyses = []
            for doc in collection.find().sort("created_at", -1).limit(10):
                recent_analyses.append({
                    "analysis_id": doc.get("analysis_id"),
                    "user_id": doc.get("user_id"),
                    "question_num": doc.get("question_num"),
                    "status": doc.get("status"),
                    "created_at": doc.get("created_at"),
                    "session_id": doc.get("session_id")
                })
            
            return {
                "timestamp": datetime.now().isoformat(),
                "total_statistics": {
                    "total_analyses": total_analyses,
                    "completed": completed_analyses,
                    "processing": processing_analyses,
                    "failed": failed_analyses,
                    "completion_rate": round(completed_analyses / total_analyses * 100, 1) if total_analyses > 0 else 0
                },
                "auto_batch_statistics": {
                    "total_auto_analyses": auto_analyses,
                    "auto_completed": auto_completed,
                    "auto_completion_rate": round(auto_completed / auto_analyses * 100, 1) if auto_analyses > 0 else 0
                },
                "recent_analyses": recent_analyses
            }
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ìë™ ë¶„ì„ ìƒíƒœ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.post("/auto-analysis/restart")
async def restart_auto_analysis():
    """
    ìë™ ë¶„ì„ì„ ìˆ˜ë™ìœ¼ë¡œ ì¬ì‹œì‘í•©ë‹ˆë‹¤.
    """
    try:
        print("ğŸ”„ ìë™ ë¶„ì„ì„ ìˆ˜ë™ìœ¼ë¡œ ì¬ì‹œì‘í•©ë‹ˆë‹¤...")
        asyncio.create_task(auto_analyze_all_s3_videos())
        
        return {
            "message": "ìë™ ë¶„ì„ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ìë™ ë¶„ì„ ì¬ì‹œì‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.get("/gpt-batch/status")
async def get_gpt_batch_status():
    """GPT ë°°ì¹˜ ì²˜ë¦¬ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    global _pending_gpt_analyses, _batch_processing_active
    
    return {
        "batch_processing_active": _batch_processing_active,
        "pending_analyses": len(_pending_gpt_analyses),
        "queue": [
            {
                "analysis_id": item["analysis_id"],
                "user_id": item["user_id"], 
                "question_num": item["question_num"],
                "added_at": item["added_at"].isoformat()
            }
            for item in _pending_gpt_analyses
        ]
    }

@app.post("/gpt-batch/trigger")
async def trigger_gpt_batch(background_tasks: BackgroundTasks):
    """ìˆ˜ë™ìœ¼ë¡œ GPT ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."""
    global _pending_gpt_analyses
    
    if not _pending_gpt_analyses:
        return {
            "status": "success",
            "message": "GPT ë¶„ì„ ëŒ€ê¸° í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.",
            "pending_count": 0
        }
    
    try:
        pending_count = len(_pending_gpt_analyses)
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ GPT ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘
        background_tasks.add_task(process_gpt_batch)
        
        return {
            "status": "success", 
            "message": f"GPT ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ì‹œì‘í–ˆìŠµë‹ˆë‹¤.",
            "pending_count": pending_count,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"GPT ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘ ì‹¤íŒ¨: {str(e)}")

@app.post("/gpt-batch/check-and-trigger")
async def check_and_trigger_gpt_batch_endpoint(background_tasks: BackgroundTasks):
    """ì˜ìƒ ë¶„ì„ ì™„ë£Œ ìƒíƒœë¥¼ í™•ì¸í•˜ê³  í•„ìš”ì‹œ GPT ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."""
    try:
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ í™•ì¸ ë° íŠ¸ë¦¬ê±°
        background_tasks.add_task(check_and_trigger_gpt_batch)
        
        return {
            "status": "success",
            "message": "GPT ë°°ì¹˜ ì²˜ë¦¬ í™•ì¸ì„ ì‹œì‘í–ˆìŠµë‹ˆë‹¤.",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"GPT ë°°ì¹˜ í™•ì¸ ì‹¤íŒ¨: {str(e)}")

# === ë‚´ë¶€ ì²˜ë¦¬ í•¨ìˆ˜ë“¤ ===

async def process_s3_user_video_analysis(
    analysis_id: str,
    s3_bucket: str,
    s3_key: str,
    user_id: str,
    question_num: str,
    session_id: Optional[str]
):
    """
    S3 ì‚¬ìš©ìë³„ ì˜ìƒ ë¶„ì„ ì›Œí¬í”Œë¡œìš°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤. (ê¸°ì¡´ ë¹„ë™ê¸° ë²„ì „)
    """
    temp_dir = None
    start_time = datetime.now()
    processing_times = {}
    
    try:
        # ë¶„ì„ ìƒíƒœë¥¼ PROCESSINGìœ¼ë¡œ ì—…ë°ì´íŠ¸
        await update_analysis_status(analysis_id, "processing", "download", 10.0)
        
        # 1. ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„± ë° S3 ë‹¤ìš´ë¡œë“œ
        stage_start = datetime.now()
        temp_dir = tempfile.mkdtemp(prefix="video_analysis_")
        video_path = await s3_handler.download_file(s3_bucket, s3_key, temp_dir)
        processing_times["download"] = (datetime.now() - stage_start).total_seconds()
        
        await update_analysis_status(analysis_id, "processing", "emotion_analysis", 30.0)
        
        # 2. ì˜ìƒ/ìŒì„± ë¶„ë¦¬ (í•„ìš”ì‹œ)
        processed_video_path = await file_processor.process_video(video_path)
        
        # 3. ê°ì • ë¶„ì„ ì‹¤í–‰
        stage_start = datetime.now()
        emotion_result = await emotion_analyzer.analyze_video(processed_video_path)
        processing_times["emotion_analysis"] = (datetime.now() - stage_start).total_seconds()
        
        await update_analysis_status(analysis_id, "processing", "eye_tracking", 60.0)
        
        # 4. ì‹œì„  ì¶”ì  ë¶„ì„ ì‹¤í–‰
        stage_start = datetime.now()
        eye_tracking_result = await eye_tracking_analyzer.analyze_video(processed_video_path, s3_key)
        processing_times["eye_tracking"] = (datetime.now() - stage_start).total_seconds()
        
        await update_analysis_status(analysis_id, "processing", "llm_analysis", 80.0)
        
        # 5. LLMìœ¼ë¡œ ì¢…í•© ë¶„ì„ ë° ì½”ë©˜íŠ¸ ìƒì„±
        stage_start = datetime.now()
        llm_comment = await gpt_analyzer.generate_comment(
            emotion_result, eye_tracking_result, analysis_id
        )
        processing_times["llm_analysis"] = (datetime.now() - stage_start).total_seconds()
        
        await update_analysis_status(analysis_id, "processing", "save_results", 95.0)
        
        # 6. ê²°ê³¼ë¥¼ MongoDBì— ì €ì¥ (ì²˜ë¦¬ ì‹œê°„ í¬í•¨)
        stage_start = datetime.now()
        total_processing_time = (datetime.now() - start_time).total_seconds()
        
        analysis_data = {
            "analysis_id": analysis_id,
            "user_id": user_id,
            "session_id": session_id,
            "s3_bucket": s3_bucket,
            "s3_key": s3_key,
            "emotion_analysis": emotion_result,
            "eye_tracking_analysis": eye_tracking_result,
            "llm_comment_id": str(llm_comment.id) if hasattr(llm_comment, 'id') else None,
            "processing_times": processing_times,
            "total_processing_time": total_processing_time,
            "created_at": start_time.isoformat(),
            "completed_at": datetime.now().isoformat(),
            "status": "completed"
        }
        
        with get_db_session() as db:
            save_analysis_result(db, analysis_data)

        
        # ì‚­ì œëœ save_analysis_summary í•¨ìˆ˜ í˜¸ì¶œ ì œê±° (ë¶„ì„ ìš”ì•½ í…Œì´ë¸” ì‚­ì œë¨)
        processing_times["save_results"] = (datetime.now() - stage_start).total_seconds()
        
        # ìµœì¢… ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
        await update_analysis_status(analysis_id, "completed", None, 100.0)
        
        # GPT ë°°ì¹˜ ë¶„ì„ íì— ì¶”ê°€ (MariaDB ì €ì¥ì„ ìœ„í•´)
        await add_to_gpt_batch_queue(analysis_id, user_id, question_num)
        print(f"ğŸ“ GPT ë¶„ì„ íì— ì¶”ê°€ë¨: {analysis_id}")
        
        # ì¦‰ì‹œ GPT ë°°ì¹˜ ì²˜ë¦¬ íŠ¸ë¦¬ê±° (ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰)
        asyncio.create_task(process_gpt_batch())
        print(f"ğŸš€ GPT ë°°ì¹˜ ì²˜ë¦¬ ì¦‰ì‹œ íŠ¸ë¦¬ê±°ë¨")
        
        print(f"ë¶„ì„ ì™„ë£Œ: {analysis_id}")
        return analysis_data
        
    except Exception as e:
        print(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({analysis_id}): {str(e)}")
        
        # ì˜¤ë¥˜ ìƒíƒœë¥¼ MongoDBì— ì €ì¥
        error_data = {
            "analysis_id": analysis_id,
            "user_id": user_id,
            "session_id": session_id,
            "s3_bucket": s3_bucket,
            "s3_key": s3_key,
            "error": str(e),
            "created_at": datetime.now().isoformat(),
            "status": "error"
        }
        
        try:
            with get_db_session() as db:
                save_analysis_result(db, error_data)
        except:
            pass  # ì˜¤ë¥˜ ì €ì¥ ì‹¤íŒ¨ëŠ” ë¬´ì‹œ
        
        return error_data
            
    finally:
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if temp_dir and os.path.exists(temp_dir):
            import shutil
            shutil.rmtree(temp_dir, ignore_errors=True)

async def update_analysis_status(analysis_id: str, status: str, stage: Optional[str] = None, progress: float = 0.0):
    """ë¶„ì„ ìƒíƒœë¥¼ DBì— ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    try:
        update_data = {
            "status": status,
            "progress_percentage": progress,
            "updated_at": datetime.now().isoformat()
        }
        
        if stage:
            update_data["current_stage"] = stage
            
        if status == "processing" and "started_at" not in update_data:
            update_data["started_at"] = datetime.now().isoformat()
        elif status == "completed":
            update_data["completed_at"] = datetime.now().isoformat()
            
        # MongoDB ì—…ë°ì´íŠ¸
        with get_db_session() as db:
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” update ì¿¼ë¦¬ ì‚¬ìš©
            print(f"ìƒíƒœ ì—…ë°ì´íŠ¸: {analysis_id} -> {status} ({stage}, {progress}%)")
            
    except Exception as e:
        print(f"âš ï¸ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ ({analysis_id}): {e}")

async def add_to_gpt_batch_queue(analysis_id: str, user_id: str, question_num: str):
    """GPT ë¶„ì„ ëŒ€ê¸°ì—´ì— ì¶”ê°€í•˜ê³ , ì¡°ê±´ ì¶©ì¡± ì‹œ ë°°ì¹˜ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    global _pending_gpt_analyses
    _pending_gpt_analyses.append({
        'analysis_id': analysis_id,
        'user_id': user_id,
        'question_num': question_num,
        'added_at': datetime.now()
    })
    print(f"ğŸ“ GPT ë¶„ì„ íì— ì¶”ê°€: {analysis_id} (ëŒ€ê¸° ì¤‘: {len(_pending_gpt_analyses)}ê°œ)")

async def process_gpt_batch():
    """
    ëŒ€ê¸° ì¤‘ì¸ GPT ë¶„ì„ ì‘ì—…ì„ ì¼ê´„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    global _pending_gpt_analyses, _batch_processing_active
    
    if _batch_processing_active or not _pending_gpt_analyses:
        return
    
    _batch_processing_active = True
    batch_to_process = _pending_gpt_analyses.copy()
    _pending_gpt_analyses = []
    
    try:
        print(f"ğŸš€ GPT ë°°ì¹˜ ë¶„ì„ ì‹œì‘: {len(batch_to_process)}ê°œ í•­ëª©")
        
        for i, item in enumerate(batch_to_process, 1):
            try:
                analysis_id = item['analysis_id']
                user_id = item['user_id']
                question_num = item['question_num']
                
                print(f"[{i}/{len(batch_to_process)}] GPT ë¶„ì„ ì‹œì‘: {analysis_id}")
                
                # MongoDBì—ì„œ ë¶„ì„ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
                with get_db_session() as db:
                    collection = db['analysis_results']
                    doc = collection.find_one({'analysis_id': analysis_id})
                
                if not doc:
                    print(f"âš ï¸ ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {analysis_id}")
                    continue
                
                emotion_result = doc.get('emotion_analysis', {})
                eye_tracking_result = doc.get('eye_tracking_analysis', {})
                
                if not emotion_result or not eye_tracking_result:
                    print(f"âš ï¸ ì˜ìƒ ë¶„ì„ ê²°ê³¼ê°€ ë¶ˆì™„ì „í•¨: {analysis_id}")
                    continue
                
                # GPT ë¶„ì„ ìˆ˜í–‰
                llm_comment = await gpt_analyzer.analyze_interview_results(
                    emotion_result, eye_tracking_result, user_id, question_num
                )
                
                # === CLI ì¶œë ¥: ë¶„ì„ ê²°ê³¼ í‘œì‹œ ===
                print(f" ======================================\n")
                print(f"\n ì „ì²´ í”¼ë“œë°±:")
                print(f"   {llm_comment.overall_feedback}") 
                print(f" ======================================\n")
                
                # MariaDB atti_score í…Œì´ë¸”ì— ì¢…í•© ì½”ë©˜íŠ¸ì™€ ì ìˆ˜ ì €ì¥
                try:
                    # MongoDBì—ì„œ ì›ë³¸ ë¶„ì„ ê²°ê³¼ì˜ ì ìˆ˜ë¥¼ ì§ì ‘ ì‚¬ìš© (ì´ë¯¸ 60:40 ë°°ì ìœ¼ë¡œ ì‚°ì¶œë¨)
                    emotion_score_60 = doc.get('emotion_analysis', {}).get('interview_score', 0)  # 60ì  ë§Œì 
                    eye_score_40 = doc.get('eye_tracking_analysis', {}).get('basic_scores', {}).get('total_eye_score', 0)  # 40ì  ë§Œì 
                    
                    # LLM ì „ì²´ í”¼ë“œë°±ì„ ì¢…í•© ì½”ë©˜íŠ¸ë¡œ ì‚¬ìš©
                    total_comment = llm_comment.overall_feedback
                    
                    # audio.answer_score ë° answer_category_result í…Œì´ë¸”ì— ë©´ì ‘íƒœë„ í‰ê°€ ì €ì¥
                    # ë¶€ì •í–‰ìœ„ ê°ì§€ ê²°ê³¼ ì¶”ì¶œ
                    eye_analysis = doc.get('eye_tracking_analysis', {})
                    suspected_copying = eye_analysis.get('analysis_summary', {}).get('total_violations', 0) >= 5
                    suspected_impersonation = eye_analysis.get('analysis_summary', {}).get('face_multiple_detected', False)
                    
                    # GPT ë¶„ì„ ê²°ê³¼ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (LLMCommentì˜ strengths/weaknesses ì‚¬ìš©)
                    strength_keywords = llm_comment.strengths if llm_comment.strengths else ['ì„±ì‹¤í•œ íƒœë„']
                    weakness_keywords = llm_comment.weaknesses if llm_comment.weaknesses else ['ê°œì„  í•„ìš”']
                    
                    gpt_analysis = {
                        'strength_keyword': '\n'.join(strength_keywords),
                        'weakness_keyword': '\n'.join(weakness_keywords)
                    }
                    
                    print(f"ğŸ” GPT í‚¤ì›Œë“œ ì¶”ì¶œ: ê°•ì ={strength_keywords}, ì•½ì ={weakness_keywords}")
                    
                    await mariadb_handler.save_interview_attitude(
                        user_id=user_id,
                        question_num=question_num,
                        emotion_score=emotion_score_60,
                        eye_score=eye_score_40,
                        suspected_copying=suspected_copying,
                        suspected_impersonation=suspected_impersonation,
                        gpt_analysis=gpt_analysis
                    )
                    
                    print(f"ğŸ’¾ MariaDB ë©´ì ‘íƒœë„ í‰ê°€ ì €ì¥ ì™„ë£Œ: {user_id}/{question_num} (ê°ì •:{emotion_score_60:.1f}, ì‹œì„ :{eye_score_40:.1f}, ì»¤ë‹:{suspected_copying}, ëŒ€ë¦¬:{suspected_impersonation})")
                    
                except Exception as mariadb_error:
                    print(f"âš ï¸ MariaDB ì €ì¥ ì‹¤íŒ¨: {mariadb_error}")
                
                # MongoDBì— LLM ê²°ê³¼ ì¶”ê°€ (ì£¼ì„ì²˜ë¦¬)
                # with get_db_session() as db:
                #     collection = db['analysis_results']
                #     collection.update_one(
                #         {'analysis_id': analysis_id},
                #         {
                #             '$set': {
                #                 'llm_comment_id': str(llm_comment.id) if hasattr(llm_comment, 'id') else None,
                #                 'llm_processed_at': datetime.now().isoformat(),
                #                 'overall_score': llm_comment.overall_score
                #             }
                #         }
                #     )
                
                print(f"[{i}/{len(batch_to_process)}] GPT ë¶„ì„ ì™„ë£Œ: {analysis_id} (ì ìˆ˜: {llm_comment.overall_score})")
                
                # ë¶„ì„ ê°„ ëŒ€ê¸° (Rate limiting)
                if i < len(batch_to_process):  # ë§ˆì§€ë§‰ì´ ì•„ë‹ˆë©´ ëŒ€ê¸°
                    await asyncio.sleep(1)
                    
            except Exception as e:
                print(f"âŒ GPT ë¶„ì„ ì‹¤íŒ¨ ({item['analysis_id']}): {str(e)}")
                continue
        
        print(f"âœ… GPT ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ: {len(batch_to_process)}ê°œ ì²˜ë¦¬")
        
    except Exception as e:
        print(f"âŒ GPT ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    finally:
        _batch_processing_active = False

async def check_and_trigger_gpt_batch():
    """
    GPT ë¶„ì„ ëŒ€ê¸°ì—´ì„ í™•ì¸í•˜ê³ , ì¡°ê±´(ê°œìˆ˜ ë˜ëŠ” ì‹œê°„)ì— ë”°ë¼ ë°°ì¹˜ë¥¼ íŠ¸ë¦¬ê±°í•©ë‹ˆë‹¤.
    """
    global _pending_gpt_analyses, _batch_processing_active
    
    # ... (í•¨ìˆ˜ ë‚´ìš©ì€ ê·¸ëŒ€ë¡œ ìœ ì§€) ...

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000) 